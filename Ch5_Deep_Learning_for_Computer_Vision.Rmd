---
title: "Ch5_Deep_Learning_for_Computer_Vision"
author: "Michael Rose"
date: "May 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(tidyverse)
```

# Introduction to Convnets

A convnet takes as input tensors of shape (image height, image width, image channels).

```{r}
# Instantiating a small convnet

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28,28,1)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")

model
```

The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network, a stack of layers. 

```{r}
# first we flatten the 3d outputs to 1d and then add a few dense layers on top
# adding a classifier on top of the convnet
model <- model %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

# check model architecture
model

# training the convnet on MNIST images
mnist <- dataset_mnist()

# set variables
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist 

# array reshape
train_images <- array_reshape(train_images, c(60000, 28, 28, 1))
train_images <- train_images/255

test_images <- array_reshape(test_images, c(10000, 28, 28, 1))
test_images <- test_images / 255

# switch to categorical variables
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)

# compile model
model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

# fit
model %>% fit(
  train_images, train_labels, 
  epochs = 5, batch_size = 64
)

# evaluate on test data

(results <- model %>% evaluate(test_images, test_labels))

```

# The convolution operation

Dense layers learn global patterns in their input feature space, whereas convolutional layers learn local patterns. This gives convnets two interesting properties:

1. The patterns they learn are translation invariant - after learning a pattern in an image it will recognize it anywhere else 
2. They can learn spatial hierarchies of patterns - e.g. first will learn local patterns, then second layer will learn larger patterns made up of the smaller patterns 

Convnets operate on 3D tensors, called feature maps with 2 spatial axis (height, width) and a depth axis (also called channels). Channels incudes a dimension of 3 if RGB or 1 if greyscale.

Convolutions are defined by 2 key parameters:
1. size of patches extracted from inputs - these are typically 3x3 or 5x5
2. Depth of the output feature map - the number of filters computed by the convolution. 


# The max pooling operation

Max pooling aggressively downsamples feature maps. E.g. the size of the feature maps is halved after every layer_max_pooling_2d operation, like 26x26 -> 13x13

# Training a convnet from scratch on a small dataset

```{r}
# copying images to training, validation, and test directories
original_dataset_dir <- "~/Desktop/School Stuff/Deep_Learning_with_R/Cats_and_Dogs/train"
base_dir <- "~/Desktop/School Stuff/Deep_Learning_with_R/Cats_and_Dogs"

train_dir <- file.path(base_dir, "train_set")
dir.create(train_dir)

validation_dir <- file.path(base_dir, "validation")
dir.create(validation_dir)

test_dir <- file.path(base_dir, "test")
dir.create(test_dir)

train_cats_dir <- file.path(train_dir, "cats")
dir.create(train_cats_dir)

train_dogs_dir <- file.path(train_dir, "dogs")
dir.create(train_dogs_dir)

validation_cats_dir <- file.path(validation_dir, "cats")
dir.create(validation_cats_dir)

validation_dogs_dir <- file.path(validation_dir, "dogs")
dir.create(validation_dogs_dir)

test_cats_dir <- file.path(test_dir, "cats")
dir.create(test_cats_dir)

test_dogs_dir <- file.path(test_dir, "dogs")
dir.create(test_dogs_dir)

fnames <- paste0("cat.", 1:50, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), file.path(train_cats_dir))

fnames <- paste0("cat.", 51:75, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), file.path(validation_cats_dir))

fnames <- paste0("cat.", 76:100, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), file.path(test_cats_dir))

fnames <- paste0("dog.", 1:50, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), file.path(train_dogs_dir))

fnames <- paste0("dog.", 51:75, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), file.path(validation_dogs_dir))

fnames <- paste0("dog.", 76:100, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), file.path(test_dogs_dir))

```


## Building the network

```{r}
model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model

# configuring the model for training

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)
```

## Data Preprocessing 

For jpg -> tensors

1. Read the picture files
2. Decode the jpeg content into RGB grids of pixels
3. Convert these into floating point tensors
4. Rescale the pixel values (between [0, 255]) to the [0,1] interval

Keras image_data_generator() function can do this for us

```{r}
# rescale all images by 1/255
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  train_dir, # target directory
  train_datagen, # training data generator
  target_size = c(150, 150), # resize all images
  batch_size = 20,
  class_mode = "binary" # because we use binary_crossentropy loss, we need binary labels
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  target_size = c(150, 150), 
  batch_size = 20,
  class_mode = "binary"
)

# output of thee generators is batches of 150x150 RGB images (shape(20, 150, 150, 3)) and binary labels(shape(20))
batch <- generator_next(train_generator)
str(batch)
```

## Fit model 

We fit the model using the the fit_generator() function.

```{r}
# fitting the model using a batch generator

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 10,
  epochs = 5,
  validation_data = validation_generator,
  validation_steps = 15
)

# saving the model

model %>% save_model_hdf5("cats_and_dogs_small_1.h5")

# plot history

plot(history)

```


## Using data augmentation

Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield believable looking images

In keras this can be done by configuring a number of random transformations to be performed on images read by an image_data_generator

```{r}
# Setting up a data augmentation configuration via image_data_generator

datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40, 
  width_shift_range = 0.2, 
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2, 
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

# displaying some randomly augmented images

frames <- list.files(train_cats_dir, full.names = TRUE)
img_path <- fnames[[3]] # chooses one image to augment

img <- image_load("/home/michael/Desktop/School Stuff/Deep_Learning_with_R/Cats_and_Dogs/train_set/dogs/dog.14.jpg", target_size = c(150,150)) # reads the image and then resizes it
img_array <- image_to_array(img) # converts it to array with shape (150,150,3)
img_array <- array_reshape(img_array, c(1, 150, 150, 3)) # reshapes it to (1, 150, 150, 3)

augmentation_generator <- flow_images_from_data( # generates batches of randomly transformed images. Loops indefinitely so you need to break the loop at some point
  img_array, 
  generator = datagen, 
  batch_size = 1
)

op <- par(mfrow = c(2,2), pty = "s", mar = c(1,0,1,0))
for (i in 1:4){
  batch <- generator_next(augmentation_generator)
  plot(as.raster(batch[1,,,]))
}
par(op)

```

Because augmenting images simply transforms existing images, it does not generate new information. Therefore we need more techniques to reduce overfitting with a small dataset

```{r}
# defining a convnet that includes dropout

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4), 
  metrics = c("acc")
)

```

## Training the convnet using data augmentation generators 

```{r}
datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE
)

test_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory( # note that validation data should NOT be augmented
  train_dir, 
  datagen, # data generator
  target_size = c(150, 150), # resize images
  batch_size = 32,
  class_mode = "binary"
)

validation_generator <- flow_images_from_directory(
  validation_dir, 
  test_datagen, 
  target_size = c(150, 150),
  batch_size = 32,
  class_mode = "binary"
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 10,
  epochs = 5,
  validation_data = validation_generator,
  validation_steps = 15
)

model %>% save_model_hdf5("cats_and_dogs_small2.h5")

history
```


# Using a pretrained convnet 

## Feature Extraction

Consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch. 
Convnets are made up of two parts: the series of convolution and pooling layers and the final densely connected classifier layer. The first part is the convolutional base of the model and feature extraction
consists of taking the convolutional base of the pretrained model and then running new data through it, then training a new classifier on top of the output.

There are a variety of pretrained models that were trained on ImageNET available natively to keras: 

1. VGG16
2. XCeption
3. Inception V3
4. ResNet50
5. VGG16
6. VGG19
7. MobileNet

```{r}
# instantiate the VGG16 model

convbase <- application_vgg16(
  weights = "imagenet", # specifies the weight checkpoint from which to initialize the model
  include_top = FALSE, # refers to including (or not) the densely connected classifier on top of the network. By default this contains the 1000 classes from imagenet. We only want 2 classes (cat, dog), so we don't include it
  input_shape = c(150, 150, 3) # shape of the image tensors that we feed to the network
)

convbase

```

At this point, there are 2 ways we can proceed: 

1. Run the convnet base over our dataset, recording its output to an array on disk and then using this data as input to a standalone, densely connected classifier. 
   This solution is fast and cheap to run because it only requires running the conv base once for every input image (conv base is most expensive part of pipeline). This won't allow us to run data augmentation though. 
   
2. Extend the model we have (conv base) by adding dense layers on top and then run the whole thing end to end on input data. This allows us to use data augmentation, because every input image goes through the convolutional base 
   every time its seen by the model. This technique is more expensive than the first. 
   
Coding the first one: 

```{r}
# Fast feature extraction without data augmentation
# we start by running instances of the previously introduced image_data_generator to extract images as arrays as well as their labels. Then we extract features from these images by calling the predict method of the model 

base_dir <- "~/Desktop/School Stuff/Deep_Learning_with_R/Cats_and_Dogs"
train_dir <- file.path(base_dir, "train_set")
validation_dir <- file.path(base_dir, "validation")
test_dir <- file.path(base_dir, "test")

datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20

extract_features <- function(directory, sample_count){
  
  features <- array(0, dim = c(sample_count, 4, 4, 512))
  labels <- array(0, dim = c(sample_count))
  
  generator <- flow_images_from_directory(
    directory = directory, 
    generator = datagen, 
    target_size = c(150, 150),
    batch_size = batch_size,
    class_mode = "binary"
  )
  
  i <- 0
  while (TRUE){
    batch <- generator_next(generator)
    inputs_batch <- batch[[1]]
    labels_batch <- batch[[2]]
    features_batch <- convbase %>% predict(inputs_batch)
    
    index_range <- ((i * batch_size)+1):((i+1) * batch_size)
    features[index_range,,,] <- features_batch
    labels[index_range] <- labels_batch
    
    i <- i + 1
    if (i * batch_size >= sample_count)
      break
  }

  list(
    features = features, 
    labels = labels
  )
}

train <- extract_features(train_dir, 200)
validation <- extract_features(validation_dir, 100)
test <- extract_features(test_dir, 100)

# extracted features are currently of shape (samples, 4, 4, 512)
# We want to feed them to a densely connected classifier, so we must first flatten to (samples, 8192)

reshape_features <- function(features){
  array_reshape(features, dim = c(nrow(features), 4*4*512))
}

train$features <- reshape_features(train$features)
validation$features <- reshape_features(validation$features)
test$features <- reshape_features(test$features)
```

Now we can define our densely connected classifier and train it on the data and labels that we just recorded

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 256, activation = "relu", input_shape = 4*4*512) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(lr = 2e-5),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- model %>% fit(
  train$features, 
  train$labels,
  epochs = 10,
  batch_size = 20,
  validation_data = list(validation$features, validation$labels)
)

plot(history)
```


Now we can use technique 2 in which we extend the convbase model and use data augmentation. This requires a GPU. It is considered intractable on a CPU. If using a CPU its better to use the first method

## Feature Extraction with Data Augmentation

Because models behave just like layers, we can add a model to a sequential model just like we would a layer

```{r}
# adding a densely connected classifier on top of the convolutional base

model <- keras_model_sequential() %>% 
  convbase %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

This model above adds an extra 2 million parameters on top of VGG16's 14 million parameters.

Before we compile and train the model, its very important to freeze the convolutional base. Freezing a layer of set of layers means preventing their weights from being updated during training. If we don't do this, then the layers that were previously learnined by the convolutional base will be modified during training. Because the dense layers on top are randomly initialized, very large weight updates would be propogated through the network effectively destroying the representations previously learned.

In keras we freeze a network using the freeze_weights() function

```{r}
cat("This is the number of trainable weights before freezing:", "The conv base:", length(model$trainable_weights), "\n")

freeze_weights(convbase)

cat("This is the number of trainable weights after freezing", "The conv base:", length(model$trainable_weights), "\n")
```

Now we can start training our model 

```{r}
# Train the model end to end with a frozen convolutional base

train_datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

test_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory( # note that validation data should not be augmented
  train_dir, 
  train_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary" # because we need binary_crossentropy loss we need binary labels
)

validation_generator <- flow_images_from_directory(
  validation_dir, 
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator, 
  steps_per_epoch = 5,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 15
)

plot(history)
```

## Fine Tuning

Fine tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model and these top layers. 
This is called fine tuning because it slightly adjusts the more abstract representations of the model being reused in order to make them more relevant for the problem at hand. 

Steps for fine tuning a network are as follows: 

1. Add your custom network on top of an already trained base network
2. Freeze the base network
3. Train the part you added
4. Unfreeze some layers in the base network
5. Jointly train both these layers and the part you added 

We already did the first 3 steps above, so we can proceed to step 4. 

```{r}
# look at pretrained network to see where we want to unfreeze
convbase

# unfreezing previously frozen layers
unfreeze_weights(convbase, from = "block3_conv1")

# now we can begin fine tuning the network with the rmsprop optimizer with a very low learning rate. We want a low learning rate to limit the magnitude of the modifications we make to the representations of the three layers that we're fine tuning

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator, 
  steps_per_epoch = 10,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 15
)


# Evaluate model on test data 

test_generator <- flow_images_from_directory(
  test_dir, 
  test_datagen, 
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

model %>% evaluate_generator(test_generator, steps = 50)

model$loss

model$acc

# test accuracy of 96.5%
```

# Wrapping up

- Convnets are the best type of machine learning models for computer vision tasks. Its possible to train one from scratch even on a very small dataset with decent results
- On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when working with image data
- its easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets
- As a complement to feature extraction, you can use fine tuning, which adapts to a new problem some sort of representations previously learned by a n existing mode. This pushes performance a bit further.

# Visualizing What Convnets Learn

## Visualizing Intermediate Activations 

This consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input.

```{r}
# load the model saved earlier
model <- convbase
model


# get an input image -a picture of a cat, not part of the images the network was trained on

img_path <- "~/Desktop/School Stuff/Deep_Learning_with_R/Cats_and_Dogs/test/cats/cat.100.jpg"
img <- image_load(img_path, target_size = c(150, 150)) # preprocesses the image into a 4d tensor
img_tensor <- image_to_array(img)
img_tensor <- array_reshape(img_tensor, c(1, 150, 150, 3))
img_tensor <- img_tensor / 255 # remember that the model was trained on inputs that were preprocessed this way
dim(img_tensor) # its shape is (1, 150, 150, 3)

# display the test picture
plot(as.raster(img_tensor[1,,,]))
```

```{r}
# instantiating a model from an input tensor and a list of output tensors
layer_outputs <- lapply(model$layers[1:8], function(layer) layer$output)
activation_model <- keras_model(inputs = model$input, outputs = layer_outputs) # creates a model that will return these outputs given the model input 

# running the model in predict mode
activations <- activation_model %>% predict(img_tensor) # returns a list of five arrays: one array per layer activation 

# activation of the first convolution layer for the cat image input
first_layer_activation <- activations[[1]]
dim(first_layer_activation)

# function to plot a channel 

plot_channel <- function(channel){
  rotate <- function(x) t(apply(x, 2, rev))
  image(rotate(channel), axes = FALSE, asp = 1, col = terrain.colors(12))
}

# plotting the second channel 
plot_channel(first_layer_activation[1,,,2])

# visualizing the seventh channel
plot_channel(first_layer_activation[1,,,7])

# Visualizing every channel in every intermediate activation
#dir.create("cat_activations")
image_size <- 58
images_per_row <- 16
for (i in 1:8) {
  
  layer_activation <- activations[[i]]
  layer_name <- model$layers[[i]]$name
 
  n_features <- dim(layer_activation)[[4]]
  n_cols <- n_features / images_per_row
 
  png(paste0("cat_activations/", i, "_", layer_name, ".png"), 
      width = image_size * images_per_row, 
      height = image_size * n_cols)
  op <- par(mfrow = c(n_cols, images_per_row), mai = rep_len(0.02, 4))
  
  for (col in 0:(n_cols-1)) {
    for (row in 0:(images_per_row-1)) {
      channel_image <- layer_activation[1,,,(col*images_per_row) + row + 1]
      plot_channel(channel_image)
    }
  }
  
  par(op)
  dev.off()
}

```

This code has been a mess. Here's a link to a working version : https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/5.4-visualizing-what-convnets-learn.nb.html

Things to note:
  1. The first layer acts as a collection of various edge detectors. At that stange, the activations retain almost all of the information present in the picture 
  2. As you go higher, the activations become increasingly abstract and less visually interpretable.  They begin to encode higher level concepts like cat ear and cat eye. 
  3. The sparsity of the activations is increasing with the depth of the layer. In the first layer, all filters are activated by the input image, but in the following layers some filters are blank.
     This means that the pattern encoded by the filter isn't found in the input image. 
     
## Visualizing convnet filters
A way to inspect the filters learned by convnets is to display the visual pattern that each filter is meant to respond to. This can be done with gradient ascent in input space - applying gradient descent to the value of the input imahes of a convnet so as to maximize the response of a specific filter, starting from a blank input image.

```{r}
# defining the loss tensor for filter visualization
model <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE
)

layer_name <- "block3_conv1"
filter_index <- 1

layer_output <- get_layer(model, layer_name)$output
loss <- k_mean(layer_output[,,,filter_index])

# obtaining the gradient of the loss with regard to the input 

grads <- k_gradients(loss, model$input)[[1]] # the call to k_gradients returns an R list of tensors (of size 1 in this case). Hence, you keep only the first element -- which is a tensor

# a non obvious trick to help the gradient descent process go smoothly is to normalize the gradient tensor by dividnign it by its L2 norm (the square root of the average of the square of the values in the tensor)
# this ensures that the magnitude of the updates done to the input image is always within the same range

grads <- grads / (k_sqrt(k_mean(k_square(grads))) + 1e-5) # add 1e-5 before dividing to avoid accidentally dividing by 0

# now we need a way to compute the value of the loss tensor and the gradient tensor, given an input image.

# fetching output values given input values
iterate <- k_function(list(model$input), list(loss, grads))

c(loss_value, grads_value) %<-% iterate(list(array(0, dim = c(1, 150, 150, 3)))) 

# at this point, we can define an R loop to do stochastic gradient descent
input_img_data <- array(runif(150*150*3), dim = c(1, 150, 150, 3)) * 20 + 128 # starts from a grey image with some noise 
step <- 1
for (i in 1:40){ # runs gradient ascent for 40 steps
  c(loss_value, grads_value) %<-% iterate(list(input_img_data)) # computes the loss value and gradient value
  input_img_data <- input_img_data + (grads_value * step) # adjusts the input image in the direction that maximizes the loss
}

# the resulting image tensor is a float tensor of shape (1, 150, 150, 3) with values that may be outside of [0, 255]. We must postprocess this to turn it into a displayable image

deprocess_image <- function(x){
  dms <- dim(x)
  
  # normalize the tensor, center on 0, ensure std dev of 1
  x <- x - mean(x)
  x <- x / (sd(x) + 1e-5)
  x <- x * 0.1
  
  # clip to [0, 1]
  x <- x + 0.5
  x <- pmax(0, pmin(x, 1))
  
  array(x, dim = dms) # return with the original image dimensions 
}
```

```{r}
# now we have all the pieces. Lets put them together into an R function that takes as input a layer name and a filter index and returns a valid image tensor representing the pattern that maximizes the activation of a specified filter
generate_pattern <- function(layer_name, filter_index, size = 150){
  
  # build a loss function that maximizes the activation of the nth filter of the layer under consideration
  layer_output <- model$get_layer(layer_name)$output
  loss <- k_mean(layer_output[,,,filter_index])
  
  # compute the gradient of the input with regard to this loss
  grads <- k_gradients(loss, model$input)[[1]]
  grads <- grads / (k_sqrt(k_mean(k_square(grads))) + 1e-5)  # normalization trick: normalize the gradient
  
  # return the loss and grads given input picture
  iterate <- k_function(list(model$input), list(loss, grads))
  
  # start from a gray image with some noise
  input_img_data <- array(runif(size * size * 3), dim = c(1, size, size, 3)) * 20 + 128
  
  # run gradient ascent for 40 steps
  step <- 1
  for (i in 1:40){
    c(loss_value, grads_value) %<-% iterate(list(input_img_data))
    input_img_data <- input_img_data + (grads_value * step)
  }
  
  img <- input_img_data[1,,,]
  deprocess_image(img)
}

```

Let's try it

```{r}
library(grid)
grid::grid.newpage()
grid.raster(generate_pattern("block3_conv1", 1))

```

```{r}
# generating a grid of all filter response patterns in a layer
library(gridExtra)
dir.create("vgg_filters")

for (layer_name in c("block2_conv1"
                     )){
  size <- 140
  png(paste0("vgg_filters/", layer_name, ".png"), 
      width = 8 * size, height = 8 * size)
  grobs <- list()
  for (i in 0:7){
    for (j in 0:7){
      pattern <- generate_pattern(layer_name, i + (j*8) + 1, size = size)
      grob <- rasterGrob(pattern, 
                         width = unit(0.9, "npc"),
                         height = unit(0.9, "npc"))
      grobs[[length(grobs)+1]] <- grob
    }
  }
  
  grid.arrange(grobs = grobs, ncol = 8)
  dev.off()
}


```

The filters from the first layer in the model (block1_conv1) encode simply directional edges and colors (or colored edges).
block2_conv2 encodes simple textures made of combinations of edges and colors
Filters in higher layers begin to resemble textures found in natural images like feathers, eye, leaves, and so on



## Visualizing heatmaps of class activation

This general category of techniques is called **class activation map (cam) visualization** . 
It consists of producing heatmaps of class activation over input images. 

This is done by taking the output feature map of a convolution layer, given an input image, and weighing every channel in that feature map by the gradient of the class with respect to the channel. 
We are weighing a spatial map of "how intensely the input image activates different channels by how important each channel is with regard to the class"

```{r}
# loading vgg16
model <- application_vgg16(weights = "imagenet") # note that you include the densely connected classifier on top; in all previous cases, you discarded it

# preprocessing an input image for VGG16
img_path <- "/home/michael/Desktop/School Stuff/Deep_Learning_with_R/elephants.jpeg"

img <- image_load(img_path, target_size = c(224, 224)) %>% 
  image_to_array() %>%  # array of shape (224, 224, 3)
  array_reshape(dim = c(1, 224, 224, 3)) %>% # adds a dimension to transform the array into a batch of size (1, 224, 224, 3)
  imagenet_preprocess_input() # preprocess the batch (this does channel-wise color normalization)

# run the pretrained network on the image 

preds <- model %>% predict(img)
imagenet_decode_predictions(preds, top = 3)[[1]]

```

To visualize which parts of the image are most african elephant like, we can set up the grad CAM process

```{r}
which.max(preds[1,])

african_elephant_output <- model$output[,387] # african elephant entry in the prediction vector

last_conv_layer <- model %>% get_layer("block5_conv3") # output feature map of the block5_conv3 layer, the last convolutional layer in VGG16

grads <- k_gradients(african_elephant_output, last_conv_layer$output)[[1]] # gradient of the african elephant class with regard to the output feature map of block5_conv3

pooled_grads <- k_means(grads, axis = c(1, 2, 3)) # vector of shape (512) where each entry is the mean intensity of the gradient over a specific feature map channel

iterate <- k_function(list(model$input), 
                      list(pooled_grads, last_conv_layer$output[1,,,]))

c(pooled_grads_value, conv_layer_output_value) %<-% iterate(list(img)) # values of these two quantites, given the sample image of two elephants

for (i in 1:512){
  conv_layer_output_value[,,i] <- conv_layer_output_value[,,i] * pooled_grads_value[[i]]
}

heatmap <- apply(conv_layer_output_value, c(1, 2), mean) # the channel wise mean of the resulting feature map is the heatmap of the class activation
```

For visualization purposes, we will also normalize the heatmap between 0 and 1. 

```{r}
# heatmap post processing 

heatmap <- pmax(heatmap, 0)
heatmap <- heatmap / max(heatmap) # normalize between 0 and 1

# write a heatmap to png file
write_heatmap <- function(heatmap, filename, width = 224, height = 224, bg = "white", col = terrain.colors(12)){
  png(filename, width = width, height = height, bg = bg)
  op = par(mar = c(0, 0, 0, 0))
  on.exit({par(op); dev.off()}, add = TRUE)
  rotate <- function(x) t(apply(x, 2, rev))
  image(rotate(heatmap), axes = FALSE, asp = 1, col = col)
}

write_heatmap(heatmap, "elephant_heatmap.png") # writes the heatmap 
```

Finally, we use the magick package to superimpose the heatmap over the original image 

```{r}
# superimposing the heatmap with the original picture
library(magick)
library(viridis)

image <- image_read(img_path) # reads the original elephant image and its geometry
info <- image_info(image)
geometry <- sprintf("%dx%d!", info$width, info$height)

pal <- col2rgb(viridis(20), alpha = TRUE) # creates a blended / transparent version of the heatmap image
alpha <- floor(seq(0, 255, length = ncol(pal)))
pal_col <- rgb(t(pal), alpha = alpha, maxColorValue = 255)
write_heatmap(heatmap, "elephant_overlay.png", width = 14, height = 14, bg = NA, col = pal_col)

image_read("elephant_overlay.png") %>% 
  image_resize(geometry, filter = "quadratic") %>% 
  image_composite(image, operator = "blend", compose_args = "20") %>% 
  plot()
```

This visualization techniques answers two important questions:
  1. Why did the network think the picture contained the elephant? 
  2. Where is the elephant located in the picture? 
  
# Summary

1. Convnets are the best tool for visual classification problems
2. Convnets work by learning a hierarchy of modular patterns and concepts to represent the visual world
3. The representations they learn are easy to inspect, not black boxes
4. Data augmentation can help fight overfitting 
5. Pretrained convnets can do feature extraction and fine tuning 
