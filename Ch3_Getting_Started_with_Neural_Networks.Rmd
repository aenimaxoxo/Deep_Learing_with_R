---
title: "Untitled"
author: "Michael Rose"
date: "April 29, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(reticulate)
library(tidyverse)
use_python('/home/michael/anaconda3/bin/python3')
```

Training a NN revolves around the following objects: 
  1. Layers, which are combined into a network (or model)
  2. Input data and corresponding targets
  3. loss function which defines the feedback signal used for learning
  4. optimizer which determines how learning proceeds
  
# Layers - The Building Blocks of Deep Learning

The typical keras workflow looks something like: 

  1. Define your training data (input tensors and target tensors)
  2. Define a network of layers (or model) that maps your inputs to your targets
  3. Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor
  4. Iterate on your training data by calling the fit() method of your model

```{r}

# step 2
# linear stacks of layers
model <- keras_model_sequential() %>% 
  layer_dense(units = 32, input_shape = c(784)) %>% 
  layer_dense(units = 10, activation = "softmax")

# same model with functional api
input_tensor <- layer_input(shape = c(784))
output_tensor <- input_tensor %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

model <- keras_model(inputs = input_tensor, outputs = output_tensor)

# with the functional api we are manipulating data tensors that the model processes and applying layers to this tensor as if they were functions

# step 3
# example using a single loss function

model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.0001),
  loss = "mse",
  metrics = c("accuracy")
)

# step 4
model %>% fit(input_tensor, target_tensor, batch_size = 128, epochs = 10)

```

# IMDB

```{python}
from keras.datasets import imdb
imdb = imdb.load_data()
# imdb_word_index = imdb.get_word_index()
```

```{r}
# %<-% is the multiassignment operator from the zeallot package. It allows us to unpack the list into a set of distinct variables
imdb <- dataset_imdb(num_words = 1000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb

# alternatively, the code above pans out to 

#imdb <- dataset_imdb(num_words = 10000)
#train_data <- imdb$train$x
#train_labels <- imdb$train$y
#test_data <- imdb$test$x
#test_labels <- imdb$test$y

```

```{r}
# show encoding indices
str(train_data[[1]])

# decode words back to english
word_index <-dataset_imdb_word_index() # named list mapping words to integer index
reverse_word_index <- names(word_index) # reverses it, mapping integer indices to words
names(reverse_word_index) <- word_index
decoded_review <- sapply(train_data[[1]], function(index) {
  word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]
  if (!is.null(word)) word else "?"
})
decoded_review

```

You can't just feed lists of integers into a neural network - we have to turn them into tensors first. Either: 
  1. Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices) 
  2. One-shot encode our lists into vectors of 0s and 1s - e.g. [3,5] is a tensor with all 0s except for 3 and 5 which are ones

```{r}
# encoding sequences into a binary matrix (choice 2)
vectorize_sequences <- function(sequences, dimension = 1000) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension) # creates an all zero matrix of shape (length(sequences), dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1 # set specific indices of results[i] to 1s
  results
}

x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)

str(x_train[1,]) # take a look at the data

# convert labels from integer to numeric
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

```{r}
# create model. returns a probability by ending the network with the sigmoid function

model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(1000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

# choose a loss function and optimizer

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# sometimes we may want to tweak the compilation a bit: 

# configure optimizer parameters: 

model %>% compile(
  optimizer = optimizer_rmsprop(lr=0.001),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# using custom losses and metrics
model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.001),
  loss = loss_binary_crossentropy, # function argument
  metrics = metric_binary_accuracy # function argument
)

```

### Validating your approach 

creating a validation set

```{r}
val_indices <- 1:1000

# create validation set of 10,000 samples from training data

x_val <- x_train[val_indices, ]
partial_x_train <- x_train[-val_indices,]
y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]

# train the model for 20 epochs (20 iterations over all samples in the x_train and y_train tensors) in mini batches of 512 samples 

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512, 
  validation_data = list(x_val, y_val)
)

plot(history)

```

Validation accuracy seems to peak at 5 epochs. Lets retrain the model

```{r}

model <- keras_model_sequential() %>% 
  layer_dense(units = 128, activation = "relu", input_shape = c(1000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

model %>% fit(x_train, y_train, epochs = 5, batch_size = 512)
results <- model %>% evaluate(x_test, y_test)

results
```

### Using a trained network to generate predictions on new data

```{r}
# generate likelihood of reviews being positive
model %>% predict(x_test[1:10,])

```

# Classifying Newswires | A multiclass classification example 

```{r}
# import data
reuters <- dataset_reuters(num_words = 1000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% reuters 

# check length
length(train_data)
length(test_data)

# look at data
train_data[[1]]

# decoding back to text
word_index <- dataset_reuters_word_index()
reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index
decoded_newswire <- sapply(train_data[[1]], function(index){
  word <- if (index >= 3) reverse_word_index[[as.character(index-3)]]
  if (!is.null(word)) word else "?"
})

decoded_newswire

# the label associated with an example is an integer between 0 and 45 
train_labels[[81]]
```

Preparing the data: 

```{r}
vectorize_sequences <- function(sequences, dimension = 1000){
  results <- matrix(0, nrow=length(sequences), ncol= dimension)
  for (i in 1:length(sequences)){
    results[i, sequences[[i]]] <- 1
  }
  results
}

# vectorize data
x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)

# to vectorize the labels we can either cast the label list as an integer tensor or we can use one hot encoding. 
# one hot encoding is categorical encoding 

to_one_hot <- function(labels, dimension = 46){
  results <- matrix(0, nrow = length(labels), ncol = dimension)
  for (i in 1:length(labels))
    results[i, labels[[i]] + 1] <- 1
  results
}

one_hot_train_labels <- to_one_hot(train_labels)
one_hot_test_labels <- to_one_hot(test_labels)

# built in way in keras
one_hot_train_labels <- to_categorical(train_labels)
one_hot_test_labels <- to_categorical(test_labels)

```

Building the network

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(1000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax") # for each input output a 46 dimensional vector, each of which encodes a class
# the softmax layer will output a probability distribution in which output[[i]] is the probability the sample belongs to class i. The 46 scores sum to 1

# use categorical_crossentropy loss function 

model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy", 
  metrics = c("accuracy")
)

```

Setting aside a validation set

```{r}
val_indices <- 1:100
x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]
y_val <- one_hot_train_labels[val_indices,]
partial_y_train <- one_hot_train_labels[-val_indices,]

# train for 20 epochs

history <- model %>% 
  fit(
    partial_x_train, 
    partial_y_train,
    epochs = 20,
    batch_size = 512, 
    validation_data = list(x_val, y_val)
  )

plot(history)

# find max validation accuracy
history$metrics$val_acc %>% which.max()
history$metrics$val_acc %>% max()

# we get overfitting after epoch 17
```

Retraining from scratch 

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(1000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")

model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 17, 
  batch_size = 512, 
  validation_data = list(x_val, y_val)
)

results <- model %>% evaluate(x_test, one_hot_test_labels)
results

# with a balanced binary classifier, a completely random accuracy would be ~ 50%, but since we have so many classes it would be much lower, roughly 17.5%

test_labels_copy <- test_labels
test_labels_copy <- sample(test_labels_copy)
length(which(test_labels == test_labels_copy)) / length(test_labels)

```

Generating predictions on new data

```{r}
# generate topic predictions for all of the test data

predictions <- model %>% predict(x_test)
dim(predictions) # each entry is a vector of length 46
sum(predictions[1,]) # coefficients of the vector sum to 1
which.max(predictions[1,]) # largest entry is the class with the highest probability
```

A different way to handle the labels and the loss

Another way to encode the labels would be to preserve their integer values. Therefore we would use a different loss function, sparse_categorical_crossentropy

```{r}
# this is the same as categorical_crossentropy, it just has a different interface
model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)
```

The importance of having sufficiently large intermediate layers

Because we have 46 classes, our layers need at least 46 dimensions.Lets see what happens if we squish it down to 4:

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(1000)) %>% 
  layer_dense(units = 4, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")

model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20, 
  epoch_size = 128,
  validation_data = list(x_val, y_val)
)

# we were supposed to see a drop in accuracy due to the model compressing information, but I'm running these models on really small sets of data for my computers sake

```

Takeaways: 

 - If you're trying to classify data points among N classes, your network should end with a dense layer of size n
 - In single label, multiclass classification problem, your network should end with a softmax activation so that it will output a probability distribution over the n output classes
 - Categorical cross_entropy is almost always the loss function you should use for such problems. It minimizes the distance between the probability distribution output by the network and the true distribution of the targets
 - There are two ways to handle labels in multiclass classification: 
  - encoding the labels via categorical encoding (also known as one hot encoding) and using categorical_crossentropy as a loss function
  - encoding the labels as integers and using the sparse_categorical_crossentripy loss function
 - If you need to classify data into a large number of categories, you should avoid creating information bottlenecks in your network due to intermediate layers that are too small 
 
# Predicting House Prices: A Regression Example 

```{r}
# import data
dataset <- dataset_boston_housing()
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset 

# look at data
str(train_data)
head(train_data)
```

Preparing the data

A widespread best practice is to do featurewise normalization. For each feature in the inpit data, you subtract the mean of the feature and divide by the standard deviation so that the feature is centered around 0 and has a unit standard deviation.
This is easily done in R using the scale() function.

```{r}
# normalize the data
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
train_data <- scale(train_data, center = mean, scale = std)
test_date <- scale(test_data, center = mean, scale = std)
```

Building the network

Due to the small size of the dataset, 404 training samples and 102 test samples , we will use a small network with two hidden layers 

```{r}
# because we will need to instantiate the same model multiple times, we will use a function to construct it
build_model <- function() {
  model <- keras_model_sequential() %>% 
    layer_dense(units = 64, activation = "relu", input_shape = dim(train_data)[[2]]) %>% 
    layer_dense(units = 64, activation = "relu") %>% 
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics = c("mae") # Mean Absolute Error is the absolute value of the difference between predictions and targets
  )
}

```

Validating using k-fold cross validation

Since we have such a small validation set (~100 samples) the variance might be high due to the choice of which samples we use for our validation set. Therefore we will use kfold CV

```{r}
k <- 4
indices <- sample(1:nrow(train_data))
folds <- cut(indices, breaks = k, labels = FALSE)

num_epochs <- 100
all_scores <- c()

for (i in 1:k){
  cat("Processing fold #", i, "\n")
  
  val_indices <- which(folds == i, arr.ind = TRUE) # prepares the validation data: data from partition #k
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  partial_train_data <- train_data[-val_indices,] # prepares the training data: data from all other partitions
  partial_train_targets <- train_targets[-val_indices]
  
  model <- build_model() # use precompiled model function
  
  model %>% fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 1, verbose = 0) # trains the model in silent mode (verbose = 0)
  
  results <- model %>% evaluate(val_data, val_targets, verbose = 0) # evaluate model on the validation data
  all_scores <- c(all_scores, results$mean_absolute_error)
}

all_scores
```

Try training for 500 epochs

```{r}
num_epochs <- 500
all_mae_histories <- NULL
for (i in 1:k){
  cat("Processing fols #", i, "\n")
  
  val_indices <- which(folds == i, arr.ind = TRUE) # Prepares the validation data, data from partition #k
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  model <- build_model()
  
  history <- model %>% fit(
    partial_train_data, partial_train_targets,
    validation_data = list(val_data, val_targets),
    epochs = num_epochs, batch_size = 1, verbose = 0
  )
  
  mae_history <- history$metrics$val_mean_absolute_error
  all_mae_histories <- rbind(all_mae_histories, mae_history)
}

average_mae_history <- data.frame(
  epoch = seq(1:ncol(all_mae_histories)),
  validation_mae = apply(all_mae_histories, 2, mean)
)

line_mae <- ggplot(average_mae_history, aes(x = epoch, y = validation_mae)) + geom_line()
smooth_mae <- ggplot(average_mae_history, aes(x = epoch, y = validation_mae)) + geom_smooth()

grid.arrange(line_mae, smooth_mae, nrow=2)

```

Training the final model

```{r}
model <- build_model()
model %>% fit(train_data, train_targets, epochs = 80, batch_size = 16, verbose = 0)
result <- model %>% evaluate(test_data, test_targets)
```

Takeaways 

- Regression is done using different loss functions than classification. Mean squared error is a loss function commonly used for regression
- Evaluation metrics to be used for regression differ from those used for classification; naturally, the concept of accuracy doesn't apply for regression. A common regression metric is mean absolute error
- When features in the input data have different ranges, each feature should be scaled independently as a preprocessing step
- When there is little data available, using kfold cross validation is a great way to reliably evaluate a model
- When little training data is available, its preferable to use a small network with few hidden layers in order to avoid severe overfitting

# Summary 

