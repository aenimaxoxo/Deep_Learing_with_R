---
title: "Fundamentals of Machine Learning"
author: "Michael Rose"
date: "May 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Holdout Validation

train on training set, tune hyper parameters on validation set, test on test set

Abstracted: 

```{r}
indices <- sample(1:nrow(data), size = 0.8 * nrow(data)) # shuffling the data is usually appropriate
evaluation_data <- data[-indices,] # validation set
training_data <- data[indices, ] # training set

model <- get_model() # see ch 3 
model %>% train(training_data) # train model on training data
validation_score <- model %>% evaluate(validation_data) # evaluate on validation data

# train final model from scratch on all non test data available
model <- get_model() 
model %>% train(data)
test_score <- model %>% evaluate(test_data)

```

If little data is available then your validation and test sets contain too few samples to be statistically representative of the data at hand. 
This can be detected if random shuffling rounds of the data before splitting end up yielding very different measures of model performance

## K-Fold Validation

Split data into K partitions of equal size. For each model i, train a model on the remaining k-1 partitions and evaluate it on partition i. Your final score is then the averages of the K scores obtained. 

Abstracted: 

```{r}
k <- 4
indices <- sample(1:nrow(data))
folds <- cut(indices, breaks = k, labels = FALSE)

validation_scores <- c()

for (i in 1:k){
  validation_indices <- which(folds == i, arr.ind = TRUE)
  validation_data <- data[validation_indices,] # selects the validation data partition
  training_data <- data[-validation_indices,] # uses the remainder of the data as training data
  
  model <- get_model() # creates a brand new instance of the model (untrained)
  model %>% train(training_data)
  results <- model %>% evaluate(validation_data)
  validation_scores <- c(validation_scores, results$accuracy)
}

validation_score <- mean(validation_scores)

model <- get_model() # train the final model on all non test data available
model %>% train(data)
results <- model %>% evaluate(test_data)
```

## Iterated K-fold validation with shuffling 

This is for situations in which there is relatively little data available and you need to evaluate your model as precisely as possible. Useful in kaggle competitions. 
Applies k-fold CV multiple times, shuffling the data every time before splitting it k ways. The final score is the average of the scores obtained at each run of K-Fold validation. 
Ends up training and evaluating P * K models (where P = num iterations), very computationally expensive

## Things to keep in mind

- Data representativeness - You want both your training and test sets to be representative of the data at hand. Therefore using random shuffling is useful
- The arrow of time - If trying to predict the future given the past, do **not** shuffle the data because then we will have a temporal leak in which the model will be trained on data from the future. Make sure trained data is coming before and test data coming after in time. 
- Check for redundancy. When there are data points that are the same in both the training data and test data it can give a falsely inflated accuracy. We should make sure the training and test sets are disjoint 

## Data preprocessing, feature engineering and feature learning

Vectorization - all inputs and targets need to be in tensors of floats or ints. The process of turning them into these tensors is called vectorization
Value normalization - it isn't safe to feed neural networks large values or data that is heterogenous because it can trigger large gradient updates that can prevent the network from converging. 
  - take small values typically in the 0-1 range (or normalize larger values into this range)
  - be homogenous - all values should be in roughly the same range
  - additionally, the following normalization practice is common: 
    - normalize each feature independently to have a mean of 0 and a standard deviation of 1 with the scale() function on both the test and training data
    - EX: mean <- apply(train_data, 2, mean)
          std <- apply(train_data, 2, sd)
          train_data <- scale(train_data, center = mean, scale = std)
          test_data <- scale(test_data, center = mean, scale = std)

Missing Values - If we expect missing values in the test data but we trained the NN on a training set without any missing values then it won't be able to properly ignore missing values. Therefore it may be worthwhile to artificially generate some missing values in order to make the model more robust. This can be done by copyingn some training samples several times, and dropping some of their features that you expect to be missing in the data. 

### Reducing the networks size

The fundamental issue in machine learning is the tension between optimization and generalization - aka we want to get the best results but we want them to generalize to new data. A model trained on more data will tend to generalize better. The process of fighting overfitting is called regularization. 

We can fight overfitting by reducing the size of the model. The general workflow is to find an appropriate size is to start with relatively few layers and parameters and then add layers until you see diminishing returns with regards to validation loss. 

```{r}
# small network 

model <- keras_model_sequential() %>% 
  layer_dense(units = 4, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 4, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

# Regular size network

model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

# high capacity network

model <- keras_model_sequential() %>% 
  layer_dense(units = 512, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

Generally the smaller network starts overfitting later than the reference network (regular size) and its performance degrades more slowly once it begins to overfit
The bigger network starts overfitting almost immediately and its validation loss is also noisier. The more capacity a network has, the more quickly it can model the training data resulting in a lower training loss, but the more susceptible it is to overfitting. 

### Adding Weight Regularization

A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of the weights more regular. This is called **weight regularization** and its done by adding to the loss function of the network a cost associated with having large weights. 
The cost comes in 2 flavors: 
  - L1 Regularization: The cost added is proportional to the absolute value of the weight coefficients. 
  - L2 Regularization: The cost added is proportional to the square of the value of the weight coefficients. This is also called weight decay in the context of neural networks. 
  
```{r}
# adding L2 weight regularization to the model 

model <- keras_model_sequential() %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001), activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001), activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

# regulizerl2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value to the total loss of the network. 
# because this penalty is only added at training time the loss for the network will be much higher at training time than at test time. 

```


### Adding Dropout

Dropout is one of the most effective and commonly used regularization techniques for neural networks.
Dropout, applied to a layer, consists of randomly dropping out (setting to 0) a number of output features of the layer during training. 
During training, given a normal vector e.g. [0.2, 0.5, 1.3, 0.8] it will randomly drop [0, 0.5, 1.3, 0]. The dropout rate is the proportion of the features zeroed out, usually between 0.2 and 0.5. 
At test time, no units are dropped out, instead the layers ouput values are scaled down by a factor equal to the dropout rate to balance for the fact that more units are active then at training time. 

```{r}
# adding dropout to the imdb network

model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

```

The most common ways to prevent overfitting in neural networks: 
  - Get more training data
  - Reduce the capacity of the network
  - add more weight regularization 
  - add dropout
  
# The Universal Workflow of Machine Learning

## Defining the problem and assembling a dataset

- What will your input data be? What are you trying to predict? 
- What type of problem is it? binary classification, multiclass classification, scalar regression, vector regression, multiclass multilabel classification, clustering, generation or reinforcement learning? 
- be aware of the hypothesis you make at this stage 
  - That your outputs can be predicted by your inputs
  - That your data is sufficient to learn the relationship
- be aware of nonstationary problems, like seasonality of trends. Seasonality can be used, but it helps to have multiple years of data and time as an input to your model 

## Choosing a measure of success 

- Define what you mean by success: accuracy? precision and recall? customer retention? This will determine what you **optimize**
- for balanced-classification problems where every class is equally likely, accuracy and area under the reciever operting characteristic curve (ROC AUC) are common metrics
- for class-imbalanced problems, precision and recall
- for ranking or multilabel classification you can use mean average precision
- a self determined metric

## Deciding on an evaluation protocol

Once we know what we are aiming for, we must establish how we will measure the current progress. Here are three common evaluation protocols:
- Maintaining a hold out validation set - the way to go when you have plenty of data
- Doing k-fold cross validation - the right choice when you have too few samples for hold out validation to be reliable
- Doing iterated k-fold validation - for performing highly accurate model evaluation when little data is available 

## Preparing your data 

For formatting a deep neural network: 
- format data as tensors
- values should be formatted to a small range, e.g. [-1,1] or [0, 1]
- If different features take values in different ranges (heterogenous data), then the data should be normalized
- You may want to do some feature engineering, especially for small data problems 

## Develop a model that does better than baseline 

If your model does better than the baseline it has statistical power. For example, in the MNIST dataset if it does better than 0.1, or in the IMDB if it does better than 0.5

Assuming things go well, we need 3 key choices to build our working model: 

- Last-layer activation: This establishes useful constraints on the networks output. For instance, the IMDB classification used sigmoid, the regression didn't use any
- Loss function: This should match the type of problem that you're trying to solve. For example, IMDB used binary_crossentropy, regression used MSE 
- optimization configuration: What optimizer will be used? What will its learning rate be? In most cases its safe to go with rmsprop with its default learning rate 

```{r}
library(tidyverse)
tribble(
  ~"Problem Type", ~"Last-Layer Activation", ~"Loss Function",
  "Binary Classification", "sigmoid", "binary_crossentropy",
  "Multiclass single label classification", "softmax", "categorical_crossentropy",
  "Multiclass multilabel classification", "sigmoid", "binary_crossentropy",
  "Regression to arbitrary values", "None", "mse",
  "Regression to values between 0 and 1", "sigmoid", "mse or binary_crossentropy"
)
```

## Scaling up: Developing a model that overfits

Once a model with statistical power is obtained, is it sufficiently powerful? Does it have enough layers and parameters to properly model the problem at hand? 

To figure out how big a model we need, we must develop a model that overfits: 
  - Add the layers
  - Make the layers bigger
  - train for more epochs
  
When you see that the models performance on the validation set begins to degrade, we have achieved overfitting. 

## Regularizing your model and tuning your hyperparameters

Repeatedly modify your model, train it, evaluate on your validation data (not the test data at this point), modify again, and repeat until its as good as it can get

Try: 
  - Adding dropout
  - Try different architectures and adding and removing layers
  - Add L1 and/or L2 Regularization
  - Try different hyperparameters (such as number of units per layer of the learning rate of the optimizer) to find an optimal configuration
  - Optionally, iterate on feature engineering: Add new features or remove features that don't seem informative
  
Everytime we use feedback from our validation model to tune our model we leak information. This is fine just a few times, but iteratively it will lead to us overfitting our validation data

After this we can train a final production model on all the available data (training and validation) and then evaluate it one last time on our test data. If our performance is significantly worse than what we
saw on our validation set then this may mean either our validation procedure wasn't reliable after all or that we began overfitting to the validation data while tuning the parameters of the model. In this case we 
may want to switch to a more reliable evaluation protocol (such as iterated k fold cross validation).

# Summary 

- Define the problem at hand and the data that will be used. 
- Choose how we will measure success 
- Determine our evaluation protocol
- Develop a model that does better than baseline 
- Develop a model that overfits 
- Regularize our model and tune its hyperparamters based on performance on validation data.