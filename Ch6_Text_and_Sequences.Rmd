---
title: "Ch6_Text_and_Sequences"
author: "Michael Rose"
date: "May 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
```

This explores deep learning models that can process text, timeseries, and sequence data in general. The two fundamental algorithms are recurrent neural networks and 1D convnets. 

Applications include: 
  - Document classification and timeseries classification, such as identifying the topic of an article or the author of a book
  - Timeseries comparisons, such as estimating how closely related two documents or stock tickers are
  - Sequence to sequence learning, such as decoding an english sentence into french
  - sentiment analysis, such as classifying the sentiment of tweets or movie reviews as positive or negative
  - timeseries forecasting, such as predicting the future weather at a certain location given recent weather data
  
# Working with text data 

We must vectorize the text into tensors. This can be done by:
  - Segmenting text into words and transforming each word into a vector
  - Segmenting text into characters and transforming each character into a vector
  - Extract n-grams of words or characters, and transform each n-gram into a vector
    n-grams are overlapping groups of multiple consecutive words or characters

## One-hot encoding of words and characters 

Consists of associating a unique integer index with every word and then turning this integer index i into a binary vector of size N (size of vocabulary); the vector is all zeros except for the ith entry, which is 1

```{r}
# word-level one-hot encoding (toy example)
samples <- c("The cat sat on the mat.", 
             "The dog ate my homework.") # initial data

token_index <- list() # builds an index of all tokens in data
for (sample in samples){
  for (word in strsplit(sample, " ")[[1]]){ # tokenizes the samples via strsplit function. IRL we should also strip punctuation and special characters from samples 
    if (!word %in% names(token_index))
      token_index[[word]] <- length(token_index) + 2 # assigns a unique index to each unique word. Index 1 not attributed to anything.
  }
}

max_length <- 10 # vectorizes the samples. We only consider the first max_length words in each sample 

results <- array(0, dim = c(length(samples),
                            max_length,
                            max(as.integer(token_index)))) # this is where we store results

for (i in 1:length(samples)){
  sample <- samples[[i]]
  words <- head(strsplit(sample, " ")[[1]], n = max_length)
  for (j in 1:length(words)){
    index <- token_index[[words[[j]]]]
    results[[i, j, index]] <- 1
  }
}

print(results)
```

```{r}
# character level one-hot encoding
samples <- c("The cat sat on the mat.",
             "The dog ate my homework.") 

ascii_tokens <- c("", sapply(as.raw(c(32:126)), rawToChar))
token_index <- c(1:length(ascii_tokens))
names(token_index) <- ascii_tokens 

max_length <- 50

results <- array(0, dim = c(length(samples), max_length, length(token_index)))

for (i in 1:length(samples)){
  sample <- samples[[i]]
  characters <- strsplit(sample, "")[[1]]
  for (j in 1:length(characters)){
    character <- characters[[j]]
    results[i, j, token_index[[character]]] <- 1
  }
}

results

```

Keras has built in utilities for doing one-hot encoding of text at the word level or character level, starting from raw text data. We should use these because the strip special characters from strings and only take into account the N most common words in the dataset to avoid dealing with very large input vector spaces. 

```{r}
# using keras for word-level one-hot encoding

samples <- c("The cat sat on the mat.",
             "The dog ate my homework.")

tokenizer <- text_tokenizer(num_words = 1000) %>% # creates a tokenizer that takes into account 1000 most common words
  fit_text_tokenizer(samples) # builds word index

sequences <- texts_to_sequences(tokenizer, samples) # turns strings into list of integer indices

one_hot_results <- texts_to_matrix(tokenizer, samples, mode = "binary") # We could also directly get the onehot binary representations. Vectorization modes other than onehot are supported by this tokenizer

word_index <- tokenizer$word_index # how we recover the word index that was computed

cat("Found", length(word_index), "unique tokens.\n")
```

A variant of one-hot encoding is called one-hot hashing. One-hot hashing is used when the number of unique tokens in a vocabulary is too large to handle explicitly. 
Instead of explicitly assigning an index to each word and keeping a reference of these indices in a dictionary, we hash words into a vector of fixed size. 
The main advantage of this method is that it does away with maintaining an explit word index, which saves memory and allows online encoding of data. 
Downside is that its susceptible to hash collisions. 

```{r}
# word level one-hot hashing 
library(hashFunction)

samples <- c("The cat sat on the mat.",
             "The dog ate my homework.")

dimensionality <- 1000 # stores the words as vectors of size 1000. If we have close to 1000 words or more, we will see many hash collisions, which decreases the accuracy of this encoding method 

max_length <- 10

results <- array(0, dim = c(length(samples), max_length, dimensionality))

for (i in 1:length(samples)) {
  sample <- samples[[i]]
  words <- head(strsplit(sample, " ")[[1]], n = max_length)
  for (j in 1:length(words)){
    index <- abs(spooky.32(words[[i]])) %% dimensionality # use hashFunction::spooky.32() to hash the word into a random integer index [0, 1000]
    results[[i, j, index]] <- 1
  }
}

```

## Using word embeddings

Another way to associate a vector with a word is the use of dense word vectors, also called word embeddings. These vectors are dense (using floats rather than ints) and very low dimensional (whereas one hot is high dimensional)

There are two ways to obtain word embeddings: 
  1. Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup we start with random word vectors and then learn word vectors in the same way we learn the weights of a neural net
  2. Load into your model word embeddings that were precomputed using a different ML task than the one we are trying to solve. These are called pretrained word embeddings
  
### Learning word embeddings with an embedding layer 

Embedding layers allows us to learn a new embedding space (word associations) with each task. This is best understood as a dictionary that maps integers indices (which stand for words) to dense vectors. Its effectively a dictionary lookup.

word index -> embedding layer -> corresponding word vector 

```{r}
# instantiating an embedding layer
# takes 2 args: number of possible tokens, and dimensionality of the embeddings
embedding_layer <- layer_embedding(input_dim = 1000, output_dim = 64) 
```

We will apply these ideas to the IMDB movie review sentiment prediction task from chapter 3. 
First we prepare the data, restricting it to the top 10000 words and cut off reviews after only 20 words. 
The network will learn 8 dimensional embeddings for each of the 10,000 words, turn the input integer sequences into embedded sequences,
flatten the tensor to 2D and train a single dense layer on top for classification. 

```{r}
# loading the IMDB data for use with an embedding layer

max_features <- 10000 # number of words to consider as features
maxlen <- 20 # cuts off text after this number of words 

imdb <- dataset_imdb(num_words = max_features) 
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb # loads the data as a list of integers 

x_train <- pad_sequences(x_train, maxlen = maxlen) # turns the list of integers into a 2D integer tensor of shape (samples, maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen) 

# using an embedding layer and classifier on the IMDB data

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 8, input_length = maxlen) %>%  # specifies the maximum input lengths to the embedding layer so you can flatten the embedded inputs. After the embedding layer, the activations have shape(samples, maxlen, 8)
  layer_flatten() %>%  # flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen * 8)
  layer_dense(units = 1, activation = "sigmoid") # adds the classifier on top

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

summary(model)

history <- model %>% fit(
  x_train, 
  y_train,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2
)

# validation accuracy of ~75.5%
```

## Using pretrained word embeddings 

When you have little data, we may not be able to use the data alone to learn an appropriate task specific embedding of our vocabulary. 
In this case, we can use a pretrained embedding space which is highly structured and exhibits useful properties (like generic aspects of language structure)

There are various precomputed databases of word embeddings that we can download and use in a keras embedding layer. 
Word2Vec, Global Vectors for Word Representation (GloVe (nlp.stanford.edu/projects/glove))

## Putting it all together: From raw text to word embeddings

```{r}
# processing the labels of the raw IMDB data

imdb_dir <- "/home/michael/Desktop/School Stuff/Deep_Learning_with_R/aclImdb"
train_dir <- file.path(imdb_dir, "train") 

labels <- c()
texts <- c()

for (label_type in c("neg", "pos")){
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}

# since we would use this technique on a small dataset, we will restrict the training data to just the first 200 samples. 

# tokenizing the text of the raw IMDB data

maxlen <- 100 # cuts off reviews after 100 words
training_samples <- 200 # trains on 200 samples
validation_samples <- 10000 # validates on 10,000 samples
max_words <- 10000 # considers only top 10,000 words of dataset

tokenizer <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)

word_index = tokenizer$word_index 
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat("Shape of label tensor:", dim(labels), "\n")

indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples+1):(training_samples + validation_samples)]

x_train <- data[training_indices,]
y_train <- labels[training_indices]

x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

```{r}
# parsing the GloVe word embeddings file 

glove_dir <- "/home/michael/Desktop/School Stuff/Deep_Learning_with_R/GloVe"
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)){
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  words <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

cat("Found", length(embeddings_index), "word vectors.\n")
```

Next we will build an embedding matrix that we can load into an embedding layer. It must be a matrix of shape (max_words, embedding_dim) where each entry i contains the embedding_dim dimensional vector for the word of index i in the reference word index (built during tokenization). 

```{r}
# preparing the GloVe word-embeddings matrix
embedding_dim <- 100

embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words){
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      embedding_matrix[index+1,] <- embedding_vector # words not found in the embedding index will be all zeros
  }
}

```


```{r}
# Model definition
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)

```

Loading the GloVe embeddings in the model 

```{r}
# loading pretrained word embeddings into the embedding layer 

get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()
```

Additionally, we will freeze the weights of the embedding layer. When parts of a model are pretrained (like our embedding layer) and parts are randomly initialized (like our classifier), the pretrained parts shouldn
t be updated during training to avoid forgetting what they already know. 

```{r}
# training and evaluation

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32, 
  validation_data = list(x_val, y_val)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5")

plot(history) #garbage model
```

Our model was pretty crap because we had very low data (200 points). We can also train the same model without loading the pretrained word embeddings and without freezing the embedded layer. 
In that case, we will learn a task specific embedding of the input tokens. This method is more powerful with large amounts of data than using a pretrained net, but we only have 200 samples in this case. 

```{r}
# training the same model without pretrained word embeddings

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

plot(history)

```

Validation set accuracy is still really bad. Working with low data is tough. Lets evaluate the model on the test data

```{r}
# tokenizing the data of the test set

test_dir <- file.path(imdb_dir, "test")

labels <- c()
texts <- c()

for (label_type in c("neg", "pos")){
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(test_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), full.names = TRUE)){
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}

sequences <- texts_to_sequences(tokenizer, texts) 
x_test <- pad_sequences(sequences, maxlen = maxlen)
y_test <- as.array(labels)
```

Load and evaluate the first model 

```{r}
model %>% 
  load_model_weights_hdf5("pre_trained_glove_model.h5") %>% 
  evaluate(x_test, y_test)
```

Wow, an accuracy of 50%! Completely useless. This would not be the case with more data

# Understanding Recurrent Neural Networks 

A recurrent neural network processes sequences by iterating through the sequence of elements and maintaining a state containing information relative to what it has seen so far. 

```{r}
# pseudocode RNN

state_t = 0 # the state at t

for (input_t in input_sequence){
  output_t <- activation(dot(W, input_t) + dot(U, state_t) + b)
  state_t <- output_t
}

# more detailed pseudocode

state_t <- 0
for (input_t in input_sequence){
  output_t <- activation(dot(W, input_t) + dot(U, state_t) + b)
  state_t <- output_t
}
```

R implementation:

```{r}
# Naive R implementation of a simple RNN

timesteps <- 100 # number of timesteps in the input sequence
input_features <- 32 # dimensionality of the input feature space
output_features <- 64 # dimensionality of the output feature space 

random_array <- function(dim){
  array(runif(prod(dim)), dim = dim)
}

inputs <- random_array(dim = c(timesteps, input_features)) # input data: random noise for the sake of the example 
state_t <- rep_len(0, length = c(output_features)) # initial state: an all 0 vector 

# create random weight matrices
W <- random_array(dim = c(output_features, input_features))
U <- random_array(dim = c(output_features, output_features))
b <- random_array(dim = c(output_features, 1))

output_sequence <- array(0, dim = c(timesteps, output_features))
for (i in 1:nrow(inputs)){
  input_t <- inputs[i,] # input_t is a vector of shape(input_features)
  tanh(as.numeric((W %*% input_t) + (U %*% state_t) + b)) # combines the input with the current state (the previous output) to obtain the current output
  output_sequence[i,] <- as.numeric(output_t) # updates the result matrix 
  state_t <- output_t # updates the state of the network for the next timestep 
}

```

A RNN is a for loop that reuses quantities computed during the previous iteration of the loop. 

RNNs are characterized by their step function, such as

```
 tanh(as.numeric((W %*% input_t) + (U %*% state_t) + b))
```

## A recurrent layer in keras

```{r}
# the process outlines above is an actual keras layer -- The difference is that layer_simple_rnn processes batches of sequences like all other keras layers, not a single sequence like the R example above  
layer_simple_rnn(units = 32)
```

Like all recurrent layers in Keras, layer_simple_rnn can be run in two different modes: 
  1. It can return the full sequences of successive outputs for each timestep (a 3D tensor of shape(batch_size, timesteps, output_features)) 
  2. Return the last output for each input sequence (a 2D tensor of shape (batch_size, output_features))
  
```{r}
# example that uses layer_simple_rnn and returns only the output at the last timestep (#2)

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32)

summary(model)

# returning the full state sequence
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE)

summary(model)

# its useful to stack several layers one after another in order to increase the representational power of the network. 
# a stacked network that returns the full state sequence for all the intermediate layers

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>% 
  layer_simple_rnn(units = 32) # last layer only returns the last output 

summary(model)

```

Now lets use a RNN on the imdb movie review classification problem

```{r}
# preparing the IMDB data
max_features <- 10000 # number of words to consider as features 
maxlen <- 500 # cuts off texts after this many words (within the max_features most common words)
batch_size <- 32

cat("Loading data...\n")
imdb <- dataset_imdb(num_words = max_features)
c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb

cat(length(input_train), "train sequences\n")
cat(length(input_test), "test sequences\n")
cat("Pad sequences (samples x time)\n")

input_train <- pad_sequences(input_train, maxlen = maxlen)
input_test <- pad_sequences(input_test, maxlen = maxlen)
cat("input_train shape:", dim(input_train), "\n")
cat("input_test shape:", dim(input_test), "\n")
```

Training a simple recurrent network using layer_embedding and layer_simple_rnn 

```{r}
# training the model with embedding and simple RNN layers 

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 32) %>% 
  layer_simple_rnn(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128, 
  validation_split = 0.2
)

plot(history)
```

In chapter 3 we got an accuracy of 88% on this data. This model peaked at 84%. Part of the problem is that our inputs only consider the first 500 words, so this RNN has less information than our earlier baseline model. Another problem is that layer_simple_rnn isn't good at processing long sequences like text. 

```{r}
which.max(history$metrics$val_acc)
history$metrics$val_acc[5]
```

## Understanding the LSTM and GRU layers

One major issue with layer_simple_rnn is that although it should theoretically be able to retain at time t information about inputs seen many steps before, in practice, such long term dependencies are impossible to learn. This is due to the vanishing gradient problem, an effect that is similar to what is observed with non-recurrent networks (feedforward networks) that are many layers deep: As you keep adding layers to a network, the network eventually becomes untrainable. The LSTM and GRU layers were designed to deal with this problem. 

The LSTM saves information from earlier timesteps for later, preventing older signals from gradually vanishing during processing. 

```{pseudo}
# pseudocode details of the LSTM architectures 

output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)

i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)
f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)
k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)

# we obtain the new carry state (the next c_t) by combining i_t, f_t, k_t
c_t+1 = i_t * f_t * k_t 
```

## A concrete LSTM example in Keras 

```{r}
# Using the LSTM layer in Keras 

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 5 ,
  batch_size = 128,
  validation_split = 0.2
)

plot(history)
summary(model)
```

88% accuracy on validation. Pretty good! This works better because the LSTM suffers much less from the vanishing gradient problem. This is the same accuracy we got from our full neural network in chapter 3, but it only had to look at the first 500 timesteps as opposed to the full sequence. 

We could get better performance if we tuned our hyperparameters such as the embeddings dimensionality or the LSTM output dimensionality. We also didn't use any regularization. The biggest problem is that analyzing the long term, global structure of the reviews isn't helpful for a sentiment analysis problem. 

# Advanced use of Recurrent Neural Networks 

The following techniques will be covered: 

- Recurrent Dropout
- Stacking recurring layers 
- bidirectional recurrent layers 

## A temperature forecasting problem 

```{r}
# looking at the data 
library(tibble)
library(readr)

data_dir <- "~/Desktop/School Stuff/Deep_Learning_with_R/climate"
fname <- file.path(data_dir, "jena_climate_2009_2016.csv")
data <- read_csv(fname)

glimpse(data)
```

```{r}
# plotting the temperature timeseries 

library(ggplot2)
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()

```

The data is recorded every 10 minutes, so we get 144 data points per day. 

```{r}
# plotting the first 10 days of temperature data 

ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()

```

Given data going as far back as lookback steps (a timestep is 10 minutes) and sampled every steps timesteps, can we predict the temperature in delay steps? 
Lookback = 1440 - observations will go back 10 days 
steps = 6 - observations will be sampled at 1 data point per hour 
delay = 144 - targets will be 24 hours in the future 

First we must do two things: 
  1. preprocess the data and normalize each value set based on their own regularization constants 
  2. Write a generator function that takes the current array of float data and yields batches of data from the recent past, along with a target temperature in the future 
  
### Generator Functions 

Generator functions are special types of functiosn that you can repeatedly call to obtain a sequence of values. For example, the following returns an infinite sequence of numbers 

```{r}
sequence_generator <- function(start){
  value <- start - 1
  function(){
    value <<- value + 1 # superassignment updates the state from within the function 
    value
  }
}

gen <- sequence_generator(10)
gen()
```

First we will convert the data frame that we read in earlier into a matrix of floats (discarding the first column which included a text timestamp)

```{r}
# converting the data into a floating point matrix
data <- data.matrix(data[,-1])

# normalizing the data 
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)

# generator yielding timeseries samples and their targets
generator <- function(data, lookback, delay, min_index, max_index, shuffle = FALSE, batch_size = 128, step = 6){
  if (is.null(max_index)) max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function(){
    if (shuffle){
      rows <- sample(c((min_index + lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index){
        i <<- min_index + lookback
      }
      rows <- c(i:min(i + batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), lookback/step, dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)){
      indices <- seq(rows[[j]] - lookback, rows[[j]], length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay, 2]
    }
    list(samples, targets)
  }
}

```

Now we will use the abstract generator function to instantiate three generators: one for training, validation, and testing. 
Training will look at the first 200,000 time points, validation will look at the following 100,000 and testing will look 
at the remainder 

```{r}
# preparing the training, validation, and test generators 

lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128

train_gen <- generator(
  data, 
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step,
  batch_size = batch_size
)

val_gen <- generator(
  data,
  lookback = lookback, 
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data, 
  lookback = lookback, 
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

val_steps <- (300000 - 200001 - lookback) / batch_size # how many steps to draw from val_gen in order to see the entire validation set
test_steps <- (nrow(data) - 300001 - lookback) / batch_size # how many steps to draw from test_gen in order to see the entire test set

```

## A common sense, non machine learning baseline 

We will choose a common sense baseline to give us an idea of how much accuracy we should expect to justify the use of a machine learning model. 
In this case we will try to guess that the next days temperature is the same as todays temperature and see what % is correct 

We will use the mean absolute error metric: mean(abs(preds - targets))

```{r}
evaluate_naive_method <- function() {
  batch_maes <- c()
  for (step in 1:val_steps){
    c(samples, targets) %<-% val_gen()
    preds <- samples[, dim(samples)[[2]], 2]
    mae <- mean(abs(preds - targets))
    batch_maes <- c(batch_maes, mae)
  }
  print(mean(batch_maes))
}

evaluate_naive_method()
```

We get a MAE of 0.2899. Because the temperature data has been normalized, this isn't directly interpretable. It translates to an average absolute error of 0.29 * temperature_std degrees celsius, which is 2.57 degrees C


## A basic machine learning approach

Before trying a large complex model like a RNN, its best to try a simple small machine learning model (such as a small densely connected network)

```{r}
# training and evaluating a densely connected model 
model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(), 
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen, 
  steps_per_epoch = 200, 
  epochs = 10,
  validation_data = val_gen, 
  validation_steps = val_steps
)

plot(history)
```

```{r}
which.min(history$metrics$val_loss)
history$metrics$val_loss[[8]]
```

Our model returned a minimum loss at epoch 8 with a loss of 0.29. This is almost exactly the same as our baseline model, which shows us that its not easy to outperform. 
In this case our hypothesis space contains many different models, but it is excessively hard to optimize for one that beats the simple, well performing baseline. 

## A first recurrent baseline 

We will be using a model which pays attention to the causality and ordering of the data. In this case we will use a GRU (Gated Recurrent Unit)

```{r}
# training and evaluating a model with layer_gru 
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen, 
  steps_per_epoch = 5,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)

plot(history)
```

```{r}
history$metrics$val_loss[[which.min(history$metrics$val_loss)]]

```

## Using recurrent dropout to fight overfitting 

It is known that applying dropout before a recurrent layer hinders learning rather than helping with regularization.  The proper way of handling dropout in recurrent networks is that the same dropout mask (the same pattern of dropped units) should be applied at every timestep instead of a dropout mask that varies randomly from timestep to timestep. 
Keras has a specific dropout type specifically for recurrent networks. 

```{r}
# training and evaluating a dropout regularized GRU based model

model <- keras_model_sequential() %>% 
  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen, 
  steps_per_epoch = 5,
  epochs = 5, 
  validation_data = val_gen,
  validation_steps = val_steps
)

plot(history)
```

```{r}
history$metrics$val_loss[[which.min(history$metrics$val_loss)]]
```

The book showed a roughly ~26% validation loss minimum, much better than my 30% loss minimum. This is due to my using 5 epochs of 5 steps as opposed to 40 epochs of 500 steps. 

## Stacking recurrent layers 

Increasing network capacity is typically done by increasing the number of units in the layers or adding more layers. Recurrent layer stacking is a classic way to do this and is what powers the google translate algorithm. Googles algorithm is a stack of 7 large LSTM layers (huge)

To stack recurrent layers in keras, all intermediate layers should return their full sequence of outputs rather than their output in the last timestep. This is done by specifying return_sequences = TRUE. 

```{r}
# training and evaluating a dropout-regularized, stacked GRU model 

model <- keras_model_sequential() %>% 
  layer_gru(units = 32,
            dropout = 0.1,
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_gru(units = 64, 
            activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen, 
  steps_per_epoch = 500, 
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)

```

## Using Bidirectional RNNs

A bidirectional RNN is a common RNN variant that offers better performance on some tasks. Its commonly used for NLP. 

In the temperature example, chronological order was very important, but this may not be the case in NLP situations. Here we are reversing the ordering of the imdb dataset

```{r}
# training and evaluating a LSTM using reversed sequences 

max_features <- 1000 # number of words to consider as features 
maxlen <- 500 # cuts off texts after this many words (among the most common words)

imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb

x_train <- lapply(x_train, rev) # reverse sequences
x_test <- lapply(x_test, rev)

x_train <- pad_sequences(x_train, maxlen = maxlen) # pads sequences
x_test <- pad_sequences(x_test, maxlen = maxlen)

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 128) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 128, 
  validation_split = 0.2
)
```

We get performance nearly identical to that of our regular chronological order LSTM network. 

A bidirectional RNN exploits the idea of looking at a model in multiple ways (chronological and reverse chronological) to improve on the performance of regular chronological order RNNs. 
To instantiate a bidirectional RNN in keras, we use the bidirectional() function which takes a recurrent layer as an argument and creates a second, seperate instance of this recurrent layeer and uses one instance for processing the input sequences in chronological order and the other instance for processing the input sequences in reversed order. 

```{r}
# training and evaluating a bidirectional LSTM 
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 32) %>% 
  bidirectional(
    layer_lstm(units = 32)
  ) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train, 
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

It performs slightly better than the regular LSTM from the previous section, achieving over 89% validation accuracy. It also overfits more quickly, which makes sense because a bidirectional RNN has twice as many parameters as a regular chronological LSTM. 

Now on the temperature prediction class: 

```{r}
# training a bidirectional gru
model <- keras_model_sequential() %>% 
  bidirectional(layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]]))) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen, 
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps 
)
```

This performs about as well as the regular layer_gru. All the predictive capacity must come from the chronological half of the network because the antichronological half is known to be severle unperforming on this task (because the recent past matters much more than the distant past in this case)

## Going even further 

Other things that could be tried to improve performance on the temperature forecasting problem: 

  1. Adjust the number of units in each recurrent layer in the stacked setup. The current choices are largely arbitrary and probably suboptimal
  2. Adjust the learning rate used by the rmsprop optimizer 
  3. Try using layer_lstm instead of layer_gru 
  4. Try using a bigger densely connected regressor on top of the recurrent layers: That is, a bigger dense layer or even a stack of layers 
  5. Don't forget to eventually run the best performing models (in terms of MAE) on the test set, otherwise we will develop architectures that are overfitting to the validation set 
  
## Wrapping up

  1. When approaching a new problem, its good to first establish common sense baselines for our metric of choice 
  2. Try simple models before expensive ones to justify the additional expense 
  3. When we have temporal data, try recurrent networks. 
  4. To use dropout on recurrent networks, we must use a time-constant dropout mask and recurrent dropout mask. These are built into keras as dropout and recurrent_dropout arguments 
  5. Stacked RNNs provide more representational power than a single RNN layer, but they are also much more expensive computationally 
  6. Bidirectional RNNs look at sequences both ways and are useful on NLP problems. 
  
# Sequence Processing with ConvNets 

Time can be treated as a spatial dimension, like the height or width of a 2D image. Therefore, 1D convnets can be competitive with RNNs on certain sequence processing problems, usually at a considerably cheaper computational cost. 

## Implementing a 1D convnet 

We will build a simple 2 layer 1D convnet and apply it to the IMDB sentiment classification task 

```{r}
# preparing the IMDB data 

max_features <- 1000
max_len <- 200

cat("Loading data...\n")
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb 

cat(length(x_train), "train sequences\n")
cat(length(x_test), "test sequences\n")

cat("Pad sequences (sample x time)\n")
x_train <- pad_sequences(x_train, maxlen = max_len)
x_test <- pad_sequences(x_test, maxlen = max_len)

cat("x_train shape:", dim(x_train), "\n")
cat("x_test shape:", dim(x_test), "\n")

```

1D convnets are structures in the same way as their 2D counterparts. They consist of a stack of layer_conv_1d and layer_max_pooling_1d, ending in either a global pooling layer or layer_flatten that turn the 3D inputs into 2D outputs that we can feed into one or more dense layers for classification or regression. 

One difference is that we can afford to use larger convulational windows with 1D convnets. 

```{r}
# training and evaluation a simple 1D convnet on the IMDB data 

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 128, input_length = max_len) %>% 
  layer_conv_1d(filters = 32, kernel_size = 7, activation = "relu") %>% 
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 32, kernel_size = 7, activation = "relu") %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dense(units = 1)

summary(model)

model %>% compile(
  optimizer = optimizer_rmsprop(lr = 1e-4),
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 5,
  batch_size = 128,
  validation_split = 0.2
)

plot(history)
```

Our validation accuracy is somewhat less than that of LSTM, but runtime is faster on both CPU and GPU. At this point we could train the model with the correct number of epochs (epoch that maxes val_acc) on the test data.

## Combining CNNs and RNNs to process long sequences 

Since 1D convnets process input patches independently, they aren't sensitive to the order of timesteps - unlike RNNs. One way to evidence the weakness is to try the 1D convnet on the temperature problem where order sensitivity is key to making good predictions. 

```{r}
# FIRST: rerun all the temperature data loading
# training and evaluating a simple 1D convnet on the Jena data 

model <- keras_model_sequential() %>% 
  layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu", input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)

plot(history)
```

The validation mae stays close to 40%. We can't even beat the baseline model using the small convnet. This is because the convnet looks for patterns -anywhere- in the training data and has no knowledge of the temporal position of a pattern that it sees.

One strategy to combine the speed and lightness of convnets with the order sensitivity of RNNs is to use a 1D convnet as a preprocessing step before a RNN. This is especially helpful when dealing with sequences that are too long to realistically be processed by RNNs. The convnet will turn the long input sequence into much shorter downsampled sequences of higher level features. Because we can manipulate longer sequences, we can either look at data from longer ago (by increasing the lookback parameter) or at high resolution time series(by decreasing the step parameter of the generator)

```{r}
# preparing higher resolution data generators for the Jena dataset

step <- 3 # previous set to 6 (1 point per hour), now 3 (1 point per 30 min)
lookback <- 720
delay <- 144 

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step
)

val_gen <- generator(
  data, 
  lookback = lookback,
  delay = delay, 
  min_index = 200001,
  max_index = 300000,
  step = step
)

test_gen <- generator(
  data,
  lookback = lookback, 
  delay = delay, 
  min_index = 300001,
  max_index = NULL, 
  step = step
)

val_steps <- (300000 - 200001 - lookback) / 128 
test_steps <- (nrow(data) - 300001 - lookback) / 128 
```


```{r}
# model combining a 1D convolutional base and a GRU layer 

model <- keras_model_sequential() %>% 
  layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu", input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>% 
  layer_gru(units = 32, dropout = 0.1, recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

summary(model)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen, 
  steps_per_epoch = 100, 
  epochs = 5, 
  validation_data = val_gen,
  validation_steps = val_steps
)

plot(history)
```

```{r}
history$metrics$val_loss[[which.min(history$metrics$val_loss)]]
```

Judging from the validation loss, this isn't as good as the regularized GRU alone, but its much faster. It looks at twice as much data as well. 

## Wrapping up

  - In the same way that 2D convnets perform well for processing visual patterns in 2D space, 1D convnets perform well for processing temporal patterns. They offer a faster alternative to RNNs on some problems, in particular NLP tasks
  - 1D convnets are structured similarly to 2D convnets
  - RNNs are extremely expensive for processing very long sequences, so it can be a good idea to make use of the computational "cheapness" of 1D convnets. We can use a 1D convnet as a preprocessing step before a RNN, shortening the sequence and extracting     useful representations for the RNN to process. 
  
# Summary 

In this chapter we covered: 
  - tokenizing text
  - word embeddings and how to use them
  - recurrent networks 
  - how to stack RNN layers and use bidirectional RNNs for powerful sequence processing models 
  - 1D convnets for sequence processing 
  - Combining 1D convnets and RNNs for long sequences 

We also know: 
  - We can use RNNs for timeseries regression, timeseries classification, anomaly detection in timeseries, and sequence labeling (such as identifying names or dates in sentences)
  - We can use 1D convnets for machine translation (Sequence to Sequence convolutional models like SliceNet), document classification, and spelling correction. 
  - If global order matters in our sequence data, then its preferable to use a recurrent network to process it. This is typically the case for timeseries, where the recent past is likely to be more information than the distant past
  - If global ordering isn't fundamentally meaningful, then 1D convnets will turn out to work at least as well as RNNs and are cheaper. This is often the case for text data, where a keyword found at the beginning of a sentence is just as meaningful as a       keyword found at the end.
  
  